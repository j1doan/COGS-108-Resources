{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: Text Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "* Tokenization\n",
    "* Stop words\n",
    "* Stemming\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "* Bag of Words\n",
    "* Term frequency\n",
    "* Inverse document frequency\n",
    "\n",
    "Tools: `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and matplotlib setup\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (17, 7)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "import seaborn as sns\n",
    "\n",
    "#improve resolution\n",
    "#comment this line if erroring on your machine/screen\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import natural language toolkit\n",
    "# if you don't already have nltk installed, uncomment and run this\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# download stopwords & punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Natural Language Processing is a whole field of study.\n",
    "\n",
    "Like most topics in this course, there are many courses solely focused on the appropriate analysis of text. We'll cover the general concepts in this course, but know you're missing lots of important details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Natural Language Toolkit (nltk)\n",
    "\n",
    "For more details on using the functionality within this package, check out the NLTK Book.\n",
    "\n",
    "0. Preface\n",
    "1. Language Processing and Python\n",
    "2. Accessing Text Corpora and Lexical Resources\n",
    "3. Processing Raw Text\n",
    "4. Writing Structured Programs\n",
    "5. Categorizing and Tagging Words\n",
    "6. Learning to Classify Text\n",
    "7. Extracting Information from Text\n",
    "8. Analyzing Sentence Structure\n",
    "9. Building Feature Based Grammars\n",
    "10. Analyzing the Meaning of Sentences\n",
    "11. Managing Linguistic Data\n",
    "12. Afterword: Facing the Language Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VADER](https://github.com/cjhutto/vaderSentiment) is a particularly helpful tool/lexicon when working with sentiments expressed in social media (tweets, online reviews, etc.)\n",
    "\n",
    "Its functionality is available through nltk, so we'll download the vader lexicon for use later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lexicon we'll be working with today\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wi21 = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_feedback_Wi21.csv')\n",
    "df_wi21.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feedback dataset - Fall 2020\n",
    "df_fa20 = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_feedback_Fa20.csv')\n",
    "df_fa20.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feedback dataset - Spring 2020\n",
    "df_sp20 = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_feedback_Sp20.csv')\n",
    "df_sp20.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feedback dataset - Winter 2020\n",
    "df_wi20 = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_feedback_Wi20.csv')\n",
    "df_wi20.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feedback dataset - Spring 2019\n",
    "df_sp19 = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_feedback_Sp19.csv')\n",
    "df_sp19.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe & Explore\n",
    "\n",
    "We'll quickly describe and explore the data to see what information we have before moving on to Text Analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Considerations\n",
    "\n",
    "* Duplicate responses?\n",
    "* PIDs for individuals in the class (typos?)\n",
    "* Missingness?\n",
    "* Reflect reality?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nonresponses\n",
    "df_wi21.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nonresponses\n",
    "df_fa20.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nonresponses\n",
    "df_sp19.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nonresponses\n",
    "df_wi20.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nonresponses\n",
    "df_sp20.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are more nonresponses in the `enjoyed_least` category than the `enjoyed_most` category. So, more people left what they enjoyed least blank than they did what they enjoyed most."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Quarters\n",
    "\n",
    "Typically, there are a few people who have what they enjoy least but don't have an enjoy most. We don't have any last quarter...but often these students' feedback is of particular interest to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 2020\n",
    "check_least = df_fa20[df_fa20['enjoyed_most'].isnull() & df_fa20['enjoyed_least'].notnull()]\n",
    "list(check_least['enjoyed_least'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring 2020\n",
    "check_least = df_sp20[df_sp20['enjoyed_most'].isnull() & df_sp20['enjoyed_least'].notnull()]\n",
    "list(check_least['enjoyed_least'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winter 2020\n",
    "check_least = df_wi20[df_wi20['enjoyed_most'].isnull() & df_wi20['enjoyed_least'].notnull()]\n",
    "list(check_least['enjoyed_least'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring 2019\n",
    "check_least = df_sp19[df_sp19['enjoyed_most'].isnull() & df_sp19['enjoyed_least'].notnull()]\n",
    "list(check_least['enjoyed_least'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data causes a problem in `nltk`, so we either get rid of individuals who didn't respond to both, or we can replace their missing data with 'No response', knowing that this text will be included in the analysis now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_no_response(df):\n",
    "    '''replace missing data in enjoyed_most/least series with string No response'''\n",
    "    \n",
    "    df['enjoyed_most'] = df['enjoyed_most'].fillna('No response')\n",
    "    df['enjoyed_least'] = df['enjoyed_least'].fillna('No response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NAs with string 'No response'\n",
    "fill_no_response(df_wi21)\n",
    "fill_no_response(df_fa20)\n",
    "fill_no_response(df_sp20)\n",
    "fill_no_response(df_sp19)\n",
    "fill_no_response(df_wi20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Plots\n",
    "\n",
    "These can give us a quick idea of students' thoughts on the course. (I didn't ask these this quarter because I added the open ended question about how you're doing.)\n",
    "\n",
    "* Time Spent\n",
    "* (Relative Difficulty)\n",
    "* (Quiz responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fa20\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "ax = sns.distplot(df['a1'], bins = 10)\n",
    "ax.axvline(df['a1'].median(), color='#2e2e2e', linestyle='--')\n",
    "plt.title('Approximately how long (hours) did you spend?', loc='left')\n",
    "ax.text(x=df['a1'].median()+2, y=0.3, s=df['a1'].median(), fontsize=14, alpha=0.75, ha='center')\n",
    "plt.xlabel('A1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "ax = sns.distplot(df['a2'], bins = 10)\n",
    "ax.axvline(df['a2'].median(), color='#2e2e2e', linestyle='--')\n",
    "ax.text(x=df['a1'].median()+2, y=0.21, s=df['a2'].median(), fontsize=14, alpha=0.75, ha='center')\n",
    "plt.xlabel('A2')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "ax = sns.distplot(df['proposal'], bins = 10)\n",
    "ax.axvline(df['proposal'].median(), color='#2e2e2e', linestyle='--')\n",
    "ax.text(x=df['proposal'].median()+2, y=0.22, s=df['proposal'].median(), fontsize=14, alpha=0.75, ha='center')\n",
    "plt.xlabel('Project Proposal')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Checks: Words of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['enjoyed_least']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_freq(df, word):\n",
    "    \"\"\"checks for frequenccy of word specified in most and least enjoyed responses\"\"\"\n",
    "    \n",
    "    most = df['enjoyed_most'].str.lower().str.contains(word).sum()/df['enjoyed_most'].notnull().sum()\n",
    "    least = df['enjoyed_least'].str.lower().str.contains(word).sum()/df['enjoyed_least'].notnull().sum()\n",
    "    \n",
    "    out = pd.DataFrame({'most_freq': [most], 'least_freq': [least]})\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for assignment\n",
    "df = df_wi21\n",
    "check_word_freq(df, 'assignment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for project in free text\n",
    "check_word_freq(df, 'project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for group in free text\n",
    "check_word_freq(df, 'group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for individual in free text\n",
    "check_word_freq(df, 'individual')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_wi21, 'quiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_fa20, 'quiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_sp20, 'quiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_wi20, 'quiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_sp19, 'quiz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_wi21, 'lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_freq(df_fa20, 'lab')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We get a quick snapshot of what's going on in COGS 108, but we really want to understand the details. To do this, analyzing the sentiment of the text is a good next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization\n",
    "\n",
    "Tokenization is the first step in analyzing text.\n",
    "\n",
    "1. Aquire text of interest\n",
    "2. Break text down (tokenize) into smaller chunks (i.e. words, bigrams, sentences, etc.)\n",
    "\n",
    "A **token** is a single entity - think of it as a building block of language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Example\n",
    "\n",
    "Here we demonstrate what a tokenized single response looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word tokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just focus on last quarter's responses\n",
    "df = df_wi21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[25,'enjoyed_most']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word = word_tokenize(df.loc[25,'enjoyed_most'])\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[25,'enjoyed_most']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize COGS108 data\n",
    "\n",
    "Using that concept we'll tokenize the words in the `enjoyed_most` and `enjoyed_least` columns for the data in our COGS108 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize most and least responses\n",
    "df['most_token'] = df['enjoyed_most'].apply(word_tokenize) \n",
    "df['least_token'] = df['enjoyed_least'].apply(word_tokenize) \n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Stop Words\n",
    "\n",
    "Stop words are words that are of less interest to your analysis.\n",
    "\n",
    "For example, you wouldn't expect the following words to be important: is, am, are, this, a, an, the, etc.\n",
    "\n",
    "By removing stopwords, you can lower the computational burden, focusing on only the words of interest.\n",
    "\n",
    "To do so in nltk, you need to create a list of stopwords and filter them from your tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# look at stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Stop Words Example\n",
    "\n",
    "Here we compare a sentence after tokenization to one that has been tokenized and had stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of removing stop words\n",
    "filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\", tokenized_word)\n",
    "print(\"Filtered Sentence:\", filtered_sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop Words: COGS108 data\n",
    "\n",
    "Using that idea, we can go ahead and remove stop words from our tokenized most and least liked tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['most_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "df['most_stop'] = df['most_token'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df['least_stop'] = df['least_token'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Lexicon Normalization (Stemming)\n",
    "\n",
    "In language, many different words come from the same root word.\n",
    "\n",
    "For example, \"intersection\", \"intersecting\", \"intersects\", and \"intersected\" are all related to the common root word - \"intersect\".\n",
    "\n",
    "Stemming is how linguistic normalization occurs - it reduces words to their root words (and chops off additional things like 'ing') - all of the above words would be reduced to their common stem \"intersect.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming Example\n",
    "\n",
    "After tokenization and removing stop words, we can get the stem for all tokens (words) in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\", filtered_sent)\n",
    "print(\"Stemmed Sentence:\", stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming: COGS108 data\n",
    "\n",
    "Here, we obtain the stem (root word) for all tokens in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['most_stem'] = df['most_stop'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "df['least_stem'] = df['least_stop'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Frequency Distribution\n",
    "\n",
    "It can be helpful to get a sense of which words are most frequent in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get series of all most and least liked words after stemming\n",
    "# note that \"No Response\" is still being included in the analysis\n",
    "most = df['most_stem'].apply(pd.Series).stack()\n",
    "least = df['least_stem'].apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FreqDist` calculates the frequency of each word in the text and we can plot the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "# calculation word frequency\n",
    "fdist_most = FreqDist(most)\n",
    "fdist_least = FreqDist(least)\n",
    "\n",
    "# remove punctuation counts\n",
    "for punc in string.punctuation:\n",
    "    del fdist_most[punc]\n",
    "    del fdist_least[punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot - top 20\n",
    "# for words in what students like most\n",
    "fdist_most.plot(20, cumulative=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sentiment Analysis!\n",
    "\n",
    "**Sentiment Analysis** quantifies the content, idea, beliefs and opinions conveyed in text.\n",
    "\n",
    "Two general approaches:\n",
    "\n",
    "1. **Lexicon-based** - count number of words in a text belonging to each sentiment (positive, negative, happy, angry, etc.)\n",
    "2. **Machine learning-based** - develop a classification model on pre-labeled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Example\n",
    "\n",
    "To get a measure of overall sentiment in our text, we'll compare our text to the VADER lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER handles:\n",
    "\n",
    "* Capitalization (great vs GREAT) & punctuation (exclamation makes more positive!)\n",
    "* Emojis and emoticons\n",
    "* Degree modifiers (extremely good vs. marginally good)\n",
    "* Contractions and conjunctions (but signals shift)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pos` + `neg` + `neu` = 1\n",
    "\n",
    "`compound` score - metric that calculates sum of all the lexicon ratings and normalizes between -1 (most extreme negative) and +1 (most extreme positive)\n",
    "\n",
    "* positive: compound >= 0.05\n",
    "* neutral: -0.05 < compound < 0.05\n",
    "* negative : compound <= -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.polarity_scores(\"The class is super cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.polarity_scores(\"The class is not super cool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.polarity_scores(\"The class is NOT super cool!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: COGS108 data\n",
    "\n",
    "Here, we will calculate the sentiment of each most liked and least liked student response from the survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of the 'sentences' (responses) from each individual\n",
    "most_list = list(df['enjoyed_most'].values)\n",
    "least_list = list(df['enjoyed_least'].values)\n",
    "most_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that will output dataframe \n",
    "# that stores sentiment information\n",
    "def get_sentiments(input_list):\n",
    "    \n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    for sentence in input_list:\n",
    "        ss = analyser.polarity_scores(sentence)\n",
    "        ss['sentence'] = sentence\n",
    "        output = output.append(ss, ignore_index=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment measures\n",
    "least_sentiments = get_sentiments(least_list)\n",
    "most_sentiments = get_sentiments(most_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: COGS108 data output\n",
    "\n",
    "After calculating the sentiment of each response, we can look at the output of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the output\n",
    "least_sentiments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the output\n",
    "most_sentiments.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with those `No response` values\n",
    "\n",
    "We've left them in there long enough. Let's remove the `No response` values before we look at any overall patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_sentiments = most_sentiments[most_sentiments['sentence'] != 'No response']\n",
    "least_sentiments = least_sentiments[least_sentiments['sentence'] != 'No response']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: COGS108 data - `describe`\n",
    "\n",
    "To get an overall sense of the values stored in each of these dataframes, we can use describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_sentiments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_sentiments.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis: COGS108 data - plotting\n",
    "\n",
    "We can compare the distribution of the compound metric between the two analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_sentiments['compound'].plot.density(label='most')\n",
    "least_sentiments['compound'].plot.density(label='least')\n",
    "plt.legend()\n",
    "plt.xlabel('Compound Sentiment Scores')\n",
    "plt.xlim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include label for boxplot\n",
    "most_sentiments['which'] = 'most'\n",
    "least_sentiments['which'] = 'least'\n",
    "# concatenate data frames together\n",
    "compound_out = pd.concat([most_sentiments, least_sentiments])\n",
    "compound_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot compound by resonse type\n",
    "sns.boxplot(data=compound_out, x='which', y='compound')\n",
    "plt.xlabel('response')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably unsurprisingly, the overall sentiment of what students like tends to be more positive than what students like less.\n",
    "\n",
    "Probably not surprising given the data and question on the survey. But, let's dig deeper into these data moving beyond sentiment analysis..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (**TF-IDF**) sets out to identify the tokens most unique to your document of interest (relative to all documents in your corpus).\n",
    "\n",
    "**Term Frequency (TF)** - counts the number of times a given word (or token, term, etc) occurs in each document divided by the number of words in that document\n",
    "\n",
    "**Inverse Document Frequency (IDF)** - weights the word by their relative frequency across documents.\n",
    "\n",
    "Words with a high TF-IDF are those with high frequency in one document & relatively low frequency in other documents\n",
    "\n",
    "For our purposes, our **corpus** will be students' responses to what they like most and least about COGS108.\n",
    "\n",
    "We'll treat this as **two separate documents**:\n",
    "\n",
    "1. What students like most\n",
    "2. What students like least"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) approach\n",
    "\n",
    "Converts the text into a co-occurrence matrix across documents within the corpus.\n",
    "\n",
    "To do this, let's get our text ready.\n",
    "\n",
    "We're going to make sure all our words are lower case, remove punctuation from each, and then provide the text (`corpus`) to `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "# lowercase text\n",
    "least = list(map(str.lower, least_list))\n",
    "most = list(map(str.lower, most_list))\n",
    "\n",
    "# remove punctuation\n",
    "for c in string.punctuation:\n",
    "    least = str(least).replace(c, \"\")\n",
    "    most = str(most).replace(c, \"\")\n",
    "\n",
    "# get list of two documents together\n",
    "corpus = [str(least), str(most)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF\n",
    "\n",
    "With our text ready for analysis, it's time to calculate TF-IDF\n",
    "\n",
    "To start our TF-IDF analysis, we'll first create a TfidfVectorizer object to transform our text data into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=2000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF: COGS108 data - calculation\n",
    "\n",
    "Here, we use our vectorizer to calculate TF-IDF across the words in our word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF-IDF\n",
    "cogs_tfidf = pd.DataFrame(tfidf.fit_transform(corpus).toarray())\n",
    "cogs_tfidf.columns = tfidf.get_feature_names()\n",
    "cogs_tfidf = cogs_tfidf.rename(index={0:'least', 1:'most'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogs_tfidf.T.iloc[30:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF: COGS108 data - output\n",
    "\n",
    "If we just want to look at the word most uniuqe in each document..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique = cogs_tfidf.idxmax(axis=1) \n",
    "most_unique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can sort by the set or words most unique to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogs_tfidf.sort_values(by='most', axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogs_tfidf.sort_values(by='least', axis=1, ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Sentiment Analysis** and **TF-IDF** are really helpful when analyzing documents and corpuses of text.\n",
    "\n",
    "But, what if, from the text itself we wanted to predict whether or not the text was likely a 'most' liked or a 'least' liked comment? We'll discuss how to do this in the coming **machine learning** lectures!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
