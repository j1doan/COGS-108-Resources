{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16: Dimensionality Reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "A mathematical process to reduce the number of random variables to consider\n",
    "\n",
    "* Reduce the dimension of quantitative data to a more manageable set of variables\n",
    "* Reduced set can then be input to reveal underlying pattens in the data and/or as inputs in a model (regression, classification, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction - Synergies\n",
    "\n",
    "* How do you control redundant degress of freedom in a useful way?\n",
    "    * **Synergies** - Coordinated movements that couple a system's degrees of freedom together to reduce control complexity\n",
    "* **Importance** - Human body has massive redundancy for a given task, and given its compliance the entire body must be actuated to perform simple movements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Use Cases for Dimensionality Reduction\n",
    "\n",
    "* Thousands of sensors used to monitor an industrial process\n",
    "    * Reducing the data from these 1000s of sensors to a few features, we can then build an interpretable model\n",
    "    * Goal: Predict process failure from sensors\n",
    "* Understanding diet around the world\n",
    "    * Amount of foods eaten among populations across the world\n",
    "    * Goal: Identify diet similarity among populations\n",
    "* Identify genetic diversity\n",
    "    * determine ancestral origins based on genetic variation\n",
    "    * Coal: Learn more about our genetic history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As An Extension of EDA\n",
    "\n",
    "* Gain insight into a set of data\n",
    "* Understand how different variables relate to one another\n",
    "\n",
    "Note: Dimensionality reduction can also be used for modeling and prediction (including PCA for example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods For Dimensionality Reduction\n",
    "\n",
    "* Projecting high-D data into a lower D-space\n",
    "* Methods\n",
    "    * PCA - Prinicple Component Analysis\n",
    "    * ICA - Independent Component Analysis\n",
    "    * CCA - Canonical Correlation Analysis\n",
    "    * Clustering\n",
    "    * FA - Factor Analysis\n",
    "    * ... and many others!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Example Methods\n",
    "\n",
    "* **PCA** - (Linear) Find projections of the data into lower dimensional space that captures most of the variations in the data\n",
    "* **ICA** - (Linear) Separate mixed additive independent signals into separate sources\n",
    "* **CCA** - (Linear) Looks for relationships between two multivariate data sets\n",
    "* **Clustering** - (Nonlinear) Uses machine learning to extract features from the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture\n",
    "\n",
    "* Mutlivariate data usually occupies a lower dimensional subspace, or a slice that captures most of the features of the data\n",
    "* The question is, how do we find that slice?\n",
    "* Typically some sort of multidimensional rotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Componenet Analysis (PCA)\n",
    "\n",
    "Key Terms:\n",
    "\n",
    "* **Principle Component (PC)** - A linear combination of the predictor variables\n",
    "* **Loadings** - The weights that transform the predictors into components (aka weights)\n",
    "* **Screeplot** - Variables of each component plotted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Combine multiple numeric predictor variables into a smaller set of variables. Each variable in the smaller set is a weighted linear combination of the original set.\n",
    "\n",
    "This smaller set of variables - the ***principle components (PCs)*** - \"explain\" most of the variability of the full set of variables... but uses many fewer dimensions to do so.\n",
    "\n",
    "The **weights (loadings)** used to form the PCs explain the relative contributions of the original variables to the new PCs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple PCA: Two Predictor Variables ($X_1$ and $X_2$)\n",
    "\n",
    "For two variables $X_{1}$ and $X_{2}$ there are two principle components $Z_i$ with i = 1 or 2\n",
    "\n",
    "$$Z_{i} = w_{x,1}X_{1} + w_{x,2}X_{2}$$\n",
    "\n",
    "$w_{i,1}$ and $w_{i,2}$: weightings (*loadings*)\n",
    "\n",
    "* Transform the original variables into principle components\n",
    "\n",
    "$Z_{i}$: The first principle component (PC1)\n",
    "\n",
    "* The linear combination that best explains the total variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Stock Price Returns for Chevron (CVX) and ExxonMobil (XOM)\n",
    "\n",
    "* PC1 and PC2 are the dotted lines on the plot\n",
    "* Each principle component is orthogonal to the other one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "If you have a dataset of 500 observations and 10,000 variables, how many PCs will be calculated?\n",
    "\n",
    "A) 2\n",
    "\n",
    "B) 10\n",
    "\n",
    "C) 500\n",
    "\n",
    "**D) 10,000**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "In a dataset with 10,000 variables, which PC explains the most variance?\n",
    "\n",
    "A) PC 1\n",
    "\n",
    "B) PC 10,000\n",
    "\n",
    "**C) Depends on the analysis**\n",
    "\n",
    "D) All explain the same amount of variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But... PCA shines when you're dealing with high-dimensional data. So we have to move beyond two predictors to many predictors.\n",
    "\n",
    "Step 1: Combine all predictors in linear combination.\n",
    "\n",
    "Step 2: Assign weights that optimize the collection of the covariation to the first PC ($Z_{1}$) (maximizes the % total variance explained).\n",
    "\n",
    "Step 3: Repeat Step 2 to generate new predictor ($Z_{2}$) (second PC) with different weights. By definition ($Z_{1}$) and ($Z_{2}$) are uncorrelated. Continue until you have as many new variables (PCs) as original predictors.\n",
    "\n",
    "Step 4: Retain as many components as needed to account for *most* of the variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Data: 5648 Days (1993-2015) x 517 Stocks\n",
    "\n",
    "In this example, we'll focus on the 16 top companies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screeplot\n",
    "\n",
    "The vernacular definition of \"scree\" is an accumulation of loose stones or rocky debris lying on a slope or at the base of a hill or cliff.\n",
    "\n",
    "In a screeplot, \"it is desirable to find a sharp reduction in the size of the eigenvalues (like a cliff), with the rest of the smaller eigenvalues constituting rubble. When the eigenvalues drop dramatically in size, an additional factor would add relatively little to the information already extracted.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of PCs 1-5\n",
    "\n",
    "* PC1: Overall stock market trend\n",
    "* PC2: Price change of energy stocks\n",
    "* PC3: Movements of Apple and Costco\n",
    "* PC4: Movements of Schlumberger to other stocks\n",
    "* PC5: Financial companies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many PCs to Select?\n",
    "\n",
    "* Option 1: Visually through the screeplot\n",
    "* Option 2: % Variance explained (i.e., 80% variance explained)\n",
    "* Option 3: Inspect loadings for an intuitive interpretation\n",
    "* Option 4: Cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Key Ideas\n",
    "\n",
    "1. PCs are linear combinations of the predictor variables (numeric data only)\n",
    "2. Calculated to minimize correlation between components (minimizes redundancy)\n",
    "3. A limited number of components will typically explain most of the variance in the outcome variable\n",
    "4. Limited set of PCs can be used in place of original predictors (dimensionality reduction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting the top principle components can reveal groupings within your data\n",
    "* A screeplot can help identify how many PCs to consider; look for the elbow in the plot\n",
    "* Loadings Plot: Project values on each PC to show how much weight they have on that PC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Diet in the UK\n",
    "\n",
    "If we look back at the raw data from Northern Ireland, the population eats way more fresh potatoes and way fewer fresh fruits, cheese, and fish. This reflects real world geography..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "Which of the following likely explains the fact that North Ireland is so far from the other countries in the first principle component?\n",
    "\n",
    "A) Amount of cereals consumed\n",
    "\n",
    "B) Geography\n",
    "\n",
    "C) Amount of liquids consumed\n",
    "\n",
    "**D) Genetic differences**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Genetics and Geography\n",
    "\n",
    "Novembre, John, et al. “Genes Mirror Geography within Europe.” Nature, vol. 456, no. 7218, Nov. 2008, pp. 98–101. www.nature.com, https://doi.org/10.1038/nature07331."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNP (Single Nucleotide Polymorphism)\n",
    "\n",
    "* Reminder: Your DNA is made of up four bases: A, T, C, and G\n",
    "* A SNP is a position in one's DNA that varies between individuals (appears in at least 1% of the population)\n",
    "    * This results from normal human variation\n",
    "    * Some contribute to disease, but many are just differences between humans\n",
    "    * these are used by companies like 23andME and Ancestry.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data: 1,387 Europeans x 500,000 SNPs\n",
    "\n",
    "* Step 1: Measure genotype at 500,000 positions (SNPs) along the genome in 1387 European individuals\n",
    "* Step 2: Calculate PCs from 500,000 SNPs\n",
    "* Step 3: Plot PC1 and PC2 (each point is an individual)\n",
    "* Step 4: Compare to the map of Europe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA on SNP data for European samples reflects geographic location of where samples came from\n",
    "* PC1 is East-West, PC2 is North-South"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "This analysis used 500,000 SNPs from 1,387 individuals. How many PCs would have been calculated?\n",
    "\n",
    "A) 2\n",
    "\n",
    "B) 10\n",
    "\n",
    "C) 1,387\n",
    "\n",
    "**D) 500,000**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "This analysis used 500,000 SNPs from 1,387 individuals. How many PCs explain geographic differences across Europe by genetic ancestry?\n",
    "\n",
    "**A) 2**\n",
    "\n",
    "B) 10\n",
    "\n",
    "C) 1,387\n",
    "\n",
    "D) 500,000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICKER QUESTION**\n",
    "\n",
    "Which of the following is not true?\n",
    "\n",
    "A) PC1 explains geographic differences from North to South\n",
    "\n",
    "B) PC2 explains geographic differences from East to West\n",
    "\n",
    "**C) The French (FR) are not genetically related to the Scottish (SCT)**\n",
    "\n",
    "D) The French (FR) are more closely related genetically to Germans (DE) than they are to the Fins (GL)\n",
    "\n",
    "E) The Spanish (ES) and Portuguese (PT) are genetically similar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA: Pros and Cons\n",
    "\n",
    "Pros:\n",
    "\n",
    "* Helps compress data; reduced storage space\n",
    "* Reduces computation time\n",
    "* Helps remove redundant features (if any)\n",
    "* Identifies outliers in the data\n",
    "\n",
    "Cons:\n",
    "\n",
    "* May lead to some amount of data loss\n",
    "* Tends to find linear correlations between variables, which is sometimes understandable\n",
    "* Fails in cases where mean and covariance are not enough to define datasets\n",
    "* May not know how many principle components to keep\n",
    "* Highly affected by outliers in the data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
